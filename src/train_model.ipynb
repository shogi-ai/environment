{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T16:28:06.006341900Z",
     "start_time": "2024-06-03T16:28:03.871830600Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.agent.shogi_agent import ShogiAgent\n",
    "from src.database.get_db import get_db\n",
    "from src.database.base import ShogiGameAction\n",
    "from src.game.ai_action import ActionTaken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "agent = ShogiAgent()\n",
    "database = get_db()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T16:28:08.246216200Z",
     "start_time": "2024-06-03T16:28:06.005342200Z"
    }
   },
   "id": "2ebf5d8b34b8854e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "version = None\n",
    "session = database.session_maker()\n",
    "rows: list[ShogiGameAction] = (\n",
    "    session.query(ShogiGameAction).filter(ShogiGameAction.version == version).all()\n",
    ")\n",
    "actions: list[ActionTaken] = [ShogiGameAction.to_action(row) for row in rows]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T16:28:35.053219Z",
     "start_time": "2024-06-03T16:28:08.248723200Z"
    }
   },
   "id": "6bd4429c186ebaa9"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6561) must match the size of tensor b (81) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 13\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m action \u001B[38;5;129;01min\u001B[39;00m actions:\n\u001B[0;32m      3\u001B[0m     agent\u001B[38;5;241m.\u001B[39mremember(\n\u001B[0;32m      4\u001B[0m         priority\u001B[38;5;241m=\u001B[39maction\u001B[38;5;241m.\u001B[39mpriority,\n\u001B[0;32m      5\u001B[0m         action\u001B[38;5;241m=\u001B[39maction\u001B[38;5;241m.\u001B[39maction,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     11\u001B[0m         next_state_valid_moves\u001B[38;5;241m=\u001B[39maction\u001B[38;5;241m.\u001B[39mnext_moves,\n\u001B[0;32m     12\u001B[0m     )\n\u001B[1;32m---> 13\u001B[0m     \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m     i \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28mprint\u001B[39m(i)\n",
      "File \u001B[1;32m~\\Desktop\\ict\\sem7\\data_challange\\environment\\src\\agent\\shogi_agent.py:255\u001B[0m, in \u001B[0;36mShogiAgent.train_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    252\u001B[0m not_done_mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(bool_array, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mbool)\n\u001B[0;32m    254\u001B[0m \u001B[38;5;66;03m# compute the expected rewards for each valid move\u001B[39;00m\n\u001B[1;32m--> 255\u001B[0m policy_action_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mq_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_valid_move_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;66;03m# get only the expected reward for the chosen move (to calculate loss against the actual reward)\u001B[39;00m\n\u001B[0;32m    258\u001B[0m policy_action_values \u001B[38;5;241m=\u001B[39m policy_action_values\u001B[38;5;241m.\u001B[39mgather(\u001B[38;5;241m1\u001B[39m, actions_tensor)\n",
      "File \u001B[1;32m~\\Desktop\\ict\\sem7\\data_challange\\environment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\ict\\sem7\\data_challange\\environment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\ict\\sem7\\data_challange\\environment\\src\\agent\\deep_q_network.py:73\u001B[0m, in \u001B[0;36mDQN.forward\u001B[1;34m(self, x, mask)\u001B[0m\n\u001B[0;32m     70\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(x)\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 73\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (6561) must match the size of tensor b (81) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for action in actions:\n",
    "    agent.remember(\n",
    "        priority=action.priority,\n",
    "        action=action.action,\n",
    "        reward=action.reward,\n",
    "        done=(action.terminated or action.truncated),\n",
    "        current_state=action.current_state,\n",
    "        current_state_valid_moves=action.current_moves,\n",
    "        next_state=action.next_state,\n",
    "        next_state_valid_moves=action.next_moves,\n",
    "    )\n",
    "    agent.train_model()\n",
    "    i += 1\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T16:28:35.852376Z",
     "start_time": "2024-06-03T16:28:35.054227400Z"
    }
   },
   "id": "53ede9863eb85b70"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
