{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.environment.env import ShogiEnv\n",
    "from src.agent.shogi_agent import ShogiAgent\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "gym.register(id=\"Shogi-v0\", entry_point=\"src.environment.env:ShogiEnv\")\n",
    "env: ShogiEnv = gym.make(\"Shogi-v0\")\n",
    "agent = ShogiAgent()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T16:35:16.890718600Z",
     "start_time": "2024-06-03T16:35:12.657370600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def play_game(environment: ShogiEnv, player: ShogiAgent) -> (float, bool, bool):\n",
    "    losses = []\n",
    "    rewards = []\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    state = environment.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        current_state = state\n",
    "\n",
    "        # Take action\n",
    "        current_state_valid_moves, _ = player.mask_and_valid_moves(environment)\n",
    "        action, mask_index = player.select_action(environment)\n",
    "        state, reward, terminated, truncated, _ = environment.step(action)\n",
    "        next_state_valid_moves, _ = player.mask_and_valid_moves(environment)\n",
    "\n",
    "        # Update the player\n",
    "        player.adaptive_e_greedy()\n",
    "        player.remember(\n",
    "            1 + reward,\n",
    "            mask_index,\n",
    "            reward,\n",
    "            (terminated or truncated),\n",
    "            current_state,\n",
    "            current_state_valid_moves,\n",
    "            state,\n",
    "            next_state_valid_moves,\n",
    "        )\n",
    "        loss = player.train_model()\n",
    "\n",
    "        rewards.append(reward)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return rewards, terminated, truncated, losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T16:35:16.897966800Z",
     "start_time": "2024-06-03T16:35:16.894400200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6561) must match the size of tensor b (81) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m----> 2\u001B[0m reward_list, _terminated, _truncated, loss_list \u001B[38;5;241m=\u001B[39m \u001B[43mplay_game\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43magent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(end \u001B[38;5;241m-\u001B[39m start)\n",
      "Cell \u001B[1;32mIn[2], line 30\u001B[0m, in \u001B[0;36mplay_game\u001B[1;34m(environment, player)\u001B[0m\n\u001B[0;32m     19\u001B[0m player\u001B[38;5;241m.\u001B[39madaptive_e_greedy()\n\u001B[0;32m     20\u001B[0m player\u001B[38;5;241m.\u001B[39mremember(\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m reward,\n\u001B[0;32m     22\u001B[0m     mask_index,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     28\u001B[0m     next_state_valid_moves,\n\u001B[0;32m     29\u001B[0m )\n\u001B[1;32m---> 30\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mplayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m rewards\u001B[38;5;241m.\u001B[39mappend(reward)\n\u001B[0;32m     33\u001B[0m losses\u001B[38;5;241m.\u001B[39mappend(loss)\n",
      "File \u001B[1;32m~\\Desktop\\ict\\sem7\\data_challange\\environment\\src\\agent\\shogi_agent.py:255\u001B[0m, in \u001B[0;36mShogiAgent.train_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    252\u001B[0m not_done_mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(bool_array, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mbool)\n\u001B[0;32m    254\u001B[0m \u001B[38;5;66;03m# compute the expected rewards for each valid move\u001B[39;00m\n\u001B[1;32m--> 255\u001B[0m policy_action_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mq_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_valid_move_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;66;03m# get only the expected reward for the chosen move (to calculate loss against the actual reward)\u001B[39;00m\n\u001B[0;32m    258\u001B[0m policy_action_values \u001B[38;5;241m=\u001B[39m policy_action_values\u001B[38;5;241m.\u001B[39mgather(\u001B[38;5;241m1\u001B[39m, actions_tensor)\n",
      "File \u001B[1;32m~\\Desktop\\ict\\sem7\\data_challange\\environment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\ict\\sem7\\data_challange\\environment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\ict\\sem7\\data_challange\\environment\\src\\agent\\deep_q_network.py:73\u001B[0m, in \u001B[0;36mDQN.forward\u001B[1;34m(self, x, mask)\u001B[0m\n\u001B[0;32m     70\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(x)\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 73\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (6561) must match the size of tensor b (81) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "reward_list, _terminated, _truncated, loss_list = play_game(env, agent)\n",
    "end = time.time()\n",
    "print(end - start)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T16:35:18.576667500Z",
     "start_time": "2024-06-03T16:35:16.902967Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "progress = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-03T16:35:18.575662200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-03T16:35:18.577667700Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    start = time.time()\n",
    "    reward_list, _terminated, _truncated, loss_list = play_game(env, agent)\n",
    "    end = time.time()\n",
    "    print(f\"{i}: {end - start}\")\n",
    "\n",
    "    progress.append(\n",
    "        {\n",
    "            \"reward\": sum(reward_list),\n",
    "            \"loss\": sum(loss_list),\n",
    "            \"terminated\": _terminated,\n",
    "            \"truncated\": _truncated,\n",
    "            \"duration\": end - start,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(progress)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.plot(df.index, df[\"reward\"], linewidth=0.1)\n",
    "ax1.set_title(\"Total rewards by game\")\n",
    "\n",
    "ax2.plot(df.index, df[\"loss\"], linewidth=0.1)\n",
    "ax2.set_title(\"Total loss by game\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-03T16:35:18.579697800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head(30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-03T16:35:18.580668400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.save_model(\"models/test_1.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T16:35:18.582667200Z",
     "start_time": "2024-06-03T16:35:18.581667200Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
